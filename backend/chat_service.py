import os
import time
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv

# ‚úÖ 1. ‡πÇ‡∏´‡∏•‡∏î Environment Variable ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏ó‡∏µ‡πà‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏Å
env_path = Path(__file__).parent / '.env'
load_dotenv(dotenv_path=env_path)

from sqlalchemy.orm import Session
from langchain_openai import ChatOpenAI
from langchain_pinecone import PineconeVectorStore
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_core.prompts import PromptTemplate

# Import Models
from .database import ChatHistory, TokenLog, ModelPricing

# ==========================================
# üß† AI Configuration (Setup ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß)
# ==========================================
INDEX_NAME = os.environ.get("PINECONE_INDEX_NAME", "docscarmencloud")

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ API Key ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡∏Å‡∏±‡∏ô Error)
if not os.environ.get("GOOGLE_API_KEY"):
    print("‚ö†Ô∏è WARNING: GOOGLE_API_KEY not found in .env, embedding might fail.")

try:
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
    vectorstore = PineconeVectorStore(index_name=INDEX_NAME, embedding=embeddings)
except Exception as e:
    print(f"‚ùå Error Initializing AI: {e}")
    vectorstore = None
    embeddings = None

# Base Prompt Template
BASE_PROMPT = """
Role: You are "Carmen" (‡∏Ñ‡∏≤‡∏£‡πå‡πÄ‡∏°‡∏ô), a professional and gentle AI Support for Carmen Software.

**Instructions:**
1. Answer based **ONLY** on the provided Context.
2. **Identify User Intent:**
   - **Case A: Capability Question ("Can I...?", "‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°?"):**
     - Start with "**‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏±‡∏ö**" or "**‡∏ó‡∏≥‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏±‡∏ö**", then explain based on context.
   - **Case B: How-to / Troubleshooting ("How to...?", "‡πÅ‡∏Å‡πâ‡∏¢‡∏±‡∏á‡πÑ‡∏á?", "‡∏ó‡∏≥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?"):**
     - **DO NOT** start with "Yes/No".
     - Start directly with the solution (e.g., "‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ô‡∏µ‡πâ ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡∏ó‡∏≥‡∏ï‡∏≤‡∏°‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ‡∏Ñ‡∏£‡∏±‡∏ö...").
     - If the Context does not contain the solution, say: "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ô‡∏µ‡πâ‡∏Ñ‡∏£‡∏±‡∏ö".

3. **Step-by-Step Guide:**
   - Extract instructions into a clear numbered list (1., 2., 3.).
   - Use Thai menu/button names if available.

4. **‚õî CRITICAL FORMAT RULES:**
   - **NO HTML TAGS:** You must NEVER use HTML tags like `<a href="...">`, `<img>`, or `<div>`.
   - **NO RELATIVE IMAGES:** Do NOT output Markdown image tags like `![image](image-44.png)`. Only include images if they are full URLs starting with `http` or `https`.
   - **MARKDOWN ONLY:** If you need to insert a link, use Markdown format: `[Link Text](URL)`.

**Extra Instructions from System:**
{prompt_extend}

**Tone:** Natural, helpful, and polite (Thai language).

Context:
{context}

Question:
{question}

Answer:
"""

# ==========================================
# üöÄ Main Service Logic
# ==========================================
async def process_chat_message(
    db: Session,
    message: str,
    bu: str,
    session_id: str = None,
    model_name: str = None,
    prompt_extend: str = "",
    theme: str = None,
    title: str = None
):
    if not vectorstore:
        return {
            "answer": "‚ö†Ô∏è ‡∏£‡∏∞‡∏ö‡∏ö AI ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô (‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö API Key)",
            "bu": bu,
            "model": "error"
        }

    start_time = time.time()
    
    # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Model (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡πà‡∏á‡∏°‡∏≤ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß Active ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î)
    if not model_name:
        active_model = db.query(ModelPricing).filter(ModelPricing.is_active == True).first()
        model_name = active_model.model_name if active_model else "xiaomi/mimo-v2-flash:free"
    
    # ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤ Model
    pricing = db.query(ModelPricing).filter(ModelPricing.model_name == model_name).first()
    input_rate = pricing.input_rate if pricing else 0.0
    output_rate = pricing.output_rate if pricing else 0.0

    # 2. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° User ‡∏•‡∏á DB ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
    user_history = ChatHistory(
        bu=bu,
        session_id=session_id,
        sender="user",
        message=message,
        model_used=model_name
    )
    db.add(user_history)
    db.commit()

    # 3. RAG: ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Pinecone (Global + BU Namespace)
    raw_results = []
    # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô BU ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
    if bu and bu != "global":
        raw_results += vectorstore.similarity_search_with_score(message, k=4, namespace=bu)
    
    # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô Global ‡πÄ‡∏™‡∏°‡∏≠
    raw_results += vectorstore.similarity_search_with_score(message, k=4, namespace="global")

    # ‡∏Å‡∏£‡∏≠‡∏á Score > 0.50
    passed_docs = [doc for doc, score in raw_results if score >= 0.50]
    
    bot_ans = ""
    usage = {}

    # 4. LLM Generation
    if not passed_docs:
        bot_ans = "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞ ‡∏â‡∏±‡∏ô‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ô‡∏µ‡πâ‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ï‡πà‡∏≥)"
    else:
        context_text = "\n\n".join([d.page_content for d in passed_docs])
        
        # Setup LLM
        llm = ChatOpenAI(
            model=model_name,
            openai_api_key=os.environ.get("OPENROUTER_API_KEY"),
            openai_api_base="https://openrouter.ai/api/v1",
            temperature=0.3
        )

        # Dynamic Prompt
        prompt = PromptTemplate(template=BASE_PROMPT, input_variables=["context", "question", "prompt_extend"])
        chain = prompt | llm

        # Run Chain
        response = await chain.ainvoke({
            "context": context_text,
            "question": message,
            "prompt_extend": prompt_extend or "None" 
        })
        
        bot_ans = response.content
        
        # Extract Token Usage
        if hasattr(response, 'response_metadata'):
            token_data = response.response_metadata.get('token_usage', {})
            usage = {
                'input_tokens': token_data.get('prompt_tokens', 0),
                'output_tokens': token_data.get('completion_tokens', 0)
            }

    # 5. Token Calculation & Fallback
    input_tk = usage.get('input_tokens', len(message) // 3) # Fallback approximate
    output_tk = usage.get('output_tokens', len(bot_ans) // 3)
    total_tk = input_tk + output_tk
    total_cost = (input_tk * input_rate) + (output_tk * output_rate)
    duration = time.time() - start_time

    # 6. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Token Log
    new_log = TokenLog(
        bu=bu,
        model_name=model_name,
        input_tokens=input_tk,
        output_tokens=output_tk,
        total_tokens=total_tk,
        cost=total_cost,
        duration=duration,
        user_query=message,
        # ‡πÄ‡∏Å‡πá‡∏ö Params ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡πÄ‡∏õ‡πá‡∏ô JSON
        additional_params={
            "theme": theme,
            "title": title,
            "prompt_extend": prompt_extend,
            "session_id": session_id
        }
    )
    db.add(new_log)

    # 7. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° Bot ‡∏•‡∏á DB
    bot_history = ChatHistory(
        bu=bu,
        session_id=session_id,
        sender="bot",
        message=bot_ans,
        model_used=model_name
    )
    db.add(bot_history)
    
    db.commit() # Commit ‡∏ó‡∏µ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ï‡∏≠‡∏ô‡∏à‡∏ö

    return {
        "answer": bot_ans,
        "bu": bu,
        "model": model_name
    }