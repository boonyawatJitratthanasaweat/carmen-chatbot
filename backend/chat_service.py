import os
import time
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv

# ‚úÖ 1. ‡πÇ‡∏´‡∏•‡∏î Environment Variable
env_path = Path(__file__).parent / '.env'
load_dotenv(dotenv_path=env_path)

from sqlalchemy.orm import Session
from langchain_openai import ChatOpenAI
from langchain_pinecone import PineconeVectorStore
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_core.prompts import PromptTemplate

# Import Models ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö Schema ‡πÉ‡∏´‡∏°‡πà
from .database import ChatHistory, TokenLog, ModelPricing

# ==========================================
# üß† AI Configuration
# ==========================================
INDEX_NAME = os.environ.get("PINECONE_INDEX_NAME", "docscarmencloud")

if not os.environ.get("GOOGLE_API_KEY"):
    print("‚ö†Ô∏è WARNING: GOOGLE_API_KEY not found")

try:
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
    vectorstore = PineconeVectorStore(index_name=INDEX_NAME, embedding=embeddings)
except Exception as e:
    print(f"‚ùå Error Initializing AI: {e}")
    vectorstore = None
    embeddings = None

# Base Prompt Template
BASE_PROMPT = """
Role: You are "Carmen" (‡∏Ñ‡∏≤‡∏£‡πå‡πÄ‡∏°‡∏ô), a professional and gentle AI Support for Carmen Software.

**Instructions:**
1. Answer based **ONLY** on the provided Context.
2. **Identify User Intent:**
   - **Case A: Capability Question ("Can I...?", "‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°?"):**
     - Start with "**‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏±‡∏ö**" or "**‡∏ó‡∏≥‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏±‡∏ö**", then explain based on context.
   - **Case B: How-to / Troubleshooting ("How to...?", "‡πÅ‡∏Å‡πâ‡∏¢‡∏±‡∏á‡πÑ‡∏á?", "‡∏ó‡∏≥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?"):**
     - **DO NOT** start with "Yes/No".
     - Start directly with the solution (e.g., "‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ô‡∏µ‡πâ ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡∏ó‡∏≥‡∏ï‡∏≤‡∏°‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ‡∏Ñ‡∏£‡∏±‡∏ö...").
     - Analyze the provided Context carefully. If the information is sufficient to answer the user's question, strictly verify the facts and provide a clear answer. Try to connect the dots if the information is fragmented.".

3. **Step-by-Step Guide:**
   - Extract instructions into a clear numbered list (1., 2., 3.).
   - Use Thai menu/button names if available.

4. **‚õî CRITICAL FORMAT RULES (Strictly Follow):**
   - **NO HTML TAGS:** You must NEVER use HTML tags like `<a href="...">`, or `<div>`.
   - **IMAGES:** If the Context contains an image path (e.g., `![alt](images/filename.png)`), **YOU MUST INCLUDE IT** in your response at the appropriate place. Do not remove it.
   - **YOUTUBE & VIDEOS:** If the context contains a YouTube URL, please output the **Raw URL** directly (e.g., `https://www.youtube.com/watch?v=...`). 
     - ‚ö†Ô∏è **DO NOT** wrap YouTube URLs in Markdown links like `[Watch Video](https://...)`. Just give the plain URL so the system can embed it.
   - **MARKDOWN ONLY:** For other links (non-video), use Markdown format: `[Link Text](URL)`.

5. **üö´ HANDLING IRRELEVANT/MISSING DATA (IMPORTANT):**
   - If the User's question is NOT related to the provided Context (e.g., weather, food, general knowledge), or if the Context is empty:
     - **DO NOT** explain what the provided context is (e.g., **NEVER SAY**: "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠...", "Based on the provided manual...").
     - **Insted, simply say:** "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏Ç‡∏≠‡∏á Carmen ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ"
     - Keep it short and polite. Do not mention "Source file" or "Manual".   

**Extra Instructions from System:**
{prompt_extend}

**Tone:** Natural, helpful, and polite (Thai language).

Context:
{context}

Question:
{question}

Answer:
"""

# ==========================================
# üöÄ Main Service Logic
# ==========================================
async def process_chat_message(
    db: Session,
    message: str,
    bu: str,
    session_id: str, # ‚úÖ ‡∏£‡∏±‡∏ö session_id ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤
    username: str,   # ‚úÖ ‡∏£‡∏±‡∏ö username ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤
    model_name: str = None,
    prompt_extend: str = "",
    theme: str = None,
    title: str = None
):
    if not vectorstore:
        return {"answer": "‚ö†Ô∏è ‡∏£‡∏∞‡∏ö‡∏ö AI ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô", "bu": bu, "model": "error"}

    start_time = time.time()
    
    # ---------------------------------------------------------
    # ‚úÖ 1. Manage Model & Pricing (FIXED)
    # ---------------------------------------------------------
    # ‡∏î‡∏∂‡∏á Active Model ‡∏à‡∏≤‡∏Å Database ‡πÄ‡∏™‡∏°‡∏≠ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Å‡∏≤‡∏£‡∏Å‡∏î Switch ‡∏°‡∏µ‡∏ú‡∏•‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
    active_model = db.query(ModelPricing).filter(ModelPricing.is_active == True).first()
    
    if active_model:
        # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ Active Model ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏•‡∏¢
        current_model_name = active_model.model_name
        input_rate = active_model.input_rate
        output_rate = active_model.output_rate
    else:
        # Fallback: ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ Active ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠ Default
        current_model_name = model_name or "xiaomi/mimo-v2-flash:free"
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡πÉ‡∏ô DB ‡πÑ‡∏´‡∏° ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà (‡∏Å‡∏±‡∏ô Error)
        pricing = db.query(ModelPricing).filter(ModelPricing.model_name == current_model_name).first()
        if not pricing:
            pricing = ModelPricing(
                model_name=current_model_name,
                input_rate=0.0,
                output_rate=0.0,
                is_active=True # Set as active since it's the fallback
            )
            db.add(pricing)
            db.commit()
            db.refresh(pricing)
        
        input_rate = pricing.input_rate
        output_rate = pricing.output_rate

    # ---------------------------------------------------------
    # 2. Save User Message to ChatHistory
    # ---------------------------------------------------------
    user_history = ChatHistory(
        session_id=session_id, # ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Session ID
        bu=bu,                 # ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏ú‡∏ô‡∏Å (‡πÄ‡∏ä‡πà‡∏ô HR)
        username=username,     # ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏ô‡πÉ‡∏ä‡πâ
        sender="user",
        message=message,
        model_used=current_model_name 
    )
    db.add(user_history)
    db.commit()

    # ---------------------------------------------------------
    # 3. RAG Search (Force Global Namespace)
    # ---------------------------------------------------------
    raw_results = []
    
    # ‚úÖ ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏à‡∏≤‡∏Å global ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏î‡∏¥‡∏°)
    raw_results += vectorstore.similarity_search_with_score(message, k=8, namespace="global")
    
    passed_docs = []
    source_debug = [] 
    
    # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡πÄ‡∏ä‡πá‡∏Ñ Score ‡πÅ‡∏•‡∏∞‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    for doc, score in raw_results:
        if score >= 0.50:
            passed_docs.append(doc)
            # ‡πÄ‡∏Å‡πá‡∏ö Metadata ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡∏Å‡∏•‡∏±‡∏ö
            source_debug.append({
                "source": doc.metadata.get("source", "Unknown"),
                "page": doc.metadata.get("page", 1),
                "score": round(float(score), 4),
                "content": doc.page_content
            })
    
    bot_ans = ""
    usage = {}

    if not passed_docs:
        bot_ans = "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡∏â‡∏±‡∏ô‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡∏Ç‡∏≠‡∏á‡∏â‡∏±‡∏ô"
    else:
        context_text = "\n\n".join([d.page_content for d in passed_docs])
        
        llm = ChatOpenAI(
            model=current_model_name, # ‚úÖ ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠ Model ‡∏ó‡∏µ‡πà Active ‡∏à‡∏£‡∏¥‡∏á
            openai_api_key=os.environ.get("OPENROUTER_API_KEY"),
            openai_api_base="https://openrouter.ai/api/v1",
            temperature=0.3
        )
        
        prompt = PromptTemplate(template=BASE_PROMPT, input_variables=["context", "question", "prompt_extend"])
        chain = prompt | llm
        
        response = await chain.ainvoke({
            "context": context_text,
            "question": message,
            "prompt_extend": prompt_extend or "None" 
        })
        
        bot_ans = response.content
        
        if hasattr(response, 'response_metadata'):
            token_data = response.response_metadata.get('token_usage', {})
            usage = {
                'input_tokens': token_data.get('prompt_tokens', 0), 
                'output_tokens': token_data.get('completion_tokens', 0)
            }

    # ---------------------------------------------------------
    # 4. Calculate Stats & Save TokenLog (FIXED CALCULATION)
    # ---------------------------------------------------------
    input_tk = usage.get('input_tokens', len(message) // 3)
    output_tk = usage.get('output_tokens', len(bot_ans) // 3)
    total_tk = input_tk + output_tk
    
    # ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏™‡∏π‡∏ï‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì: (Token / 1,000,000) * Rate
    total_cost = (input_tk / 1_000_000 * input_rate) + (output_tk / 1_000_000 * output_rate)
    
    duration = time.time() - start_time

    new_log = TokenLog(
        session_id=session_id, 
        bu=bu,                 
        username=username,     
        model_name=current_model_name, # ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ä‡∏∑‡πà‡∏≠ Model ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏à‡∏£‡∏¥‡∏á
        input_tokens=input_tk,
        output_tokens=output_tk,
        total_tokens=total_tk,
        cost=total_cost, # ‚úÖ ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß
        duration=duration,
        user_query=message,
        sources=source_debug
    )
    db.add(new_log)

    # ---------------------------------------------------------
    # 5. Save Bot Message to ChatHistory
    # ---------------------------------------------------------
    bot_history = ChatHistory(
        session_id=session_id, 
        bu=bu,
        username=username,
        sender="bot",
        message=bot_ans,
        model_used=current_model_name 
    )
    db.add(bot_history)
    db.commit()
    db.refresh(bot_history)

    return {
        "answer": bot_ans,
        "bu": bu,
        "model": current_model_name,
        "sources": source_debug,
        "message_id": bot_history.id,
        "session_id": session_id 
    }