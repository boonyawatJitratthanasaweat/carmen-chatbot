import time
from langchain_community.document_loaders import GoogleDriveLoader
import shutil 
import os
from datetime import datetime, timedelta 
from fastapi import FastAPI, HTTPException, BackgroundTasks, Depends, Request, Form, File, UploadFile
from fastapi.security import OAuth2PasswordRequestForm
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from sqlalchemy.orm import Session
from sqlalchemy import desc
import uvicorn
import io
import pandas as pd
from pathlib import Path

from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload

# AI & LangChain
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_pinecone import PineconeVectorStore
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from langchain.schema import Document
from github import Github

from dotenv import load_dotenv

from langchain_community.document_loaders import WebBaseLoader
import validators 

# ‡πÄ‡∏û‡∏¥‡πà‡∏° RecursiveUrlLoader ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤
from langchain_community.document_loaders import RecursiveUrlLoader
from bs4 import BeautifulSoup as Soup 

# Import ‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏∞‡∏ö‡∏ö
# ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° TokenLog ‡πÅ‡∏•‡∏∞ SessionLocal ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤
from .auth import get_db, create_access_token, get_current_user, get_password_hash, User as UserModel, ChatHistory
from .database import Base, engine, SessionLocal, TokenLog 

# ‡πÇ‡∏´‡∏•‡∏î ENV
env_path = Path(__file__).parent / '.env'
load_dotenv(dotenv_path=env_path)

# --- ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÉ‡∏ô Database ‡∏´‡∏•‡∏±‡∏Å (Postgres/SQLAlchemy) ---
# ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á token_logs ‡πÉ‡∏´‡πâ‡πÄ‡∏≠‡∏á‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
Base.metadata.create_all(bind=engine)

# ==========================================
# üõ†Ô∏è ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Log (PostgreSQL Version)
# ==========================================
def log_token_usage(namespace: str, model: str, input_tk: int, output_tk: int, query: str = ""):
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ Token ‡∏•‡∏á Postgres"""
    db = SessionLocal() # ‡πÄ‡∏õ‡∏¥‡∏î Connection ‡πÉ‡∏´‡∏°‡πà
    try:
        total_tk = input_tk + output_tk
        
        new_log = TokenLog(
            namespace=namespace,
            model_name=model,
            input_tokens=input_tk,
            output_tokens=output_tk,
            total_tokens=total_tk,
            user_query=query,
            timestamp=datetime.now()
        )
        
        db.add(new_log)
        db.commit()
        print(f"üìù Token Log saved: {total_tk} tokens (Client: {namespace})")
        
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to save token log: {e}")
    finally:
        db.close() # ‡∏õ‡∏¥‡∏î Connection ‡πÄ‡∏™‡∏°‡∏≠

# ==========================================

# --- Config ---
INDEX_NAME = os.environ.get("PINECONE_INDEX_NAME", "docscarmencloud")

# --- ‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏°‡∏≠‡∏á AI ---
print("üß† Loading AI Brain...")
try:
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
    vectorstore = PineconeVectorStore(index_name=INDEX_NAME, embedding=embeddings)
    
    # ‚úÖ ‡πÉ‡∏ä‡πâ ChatGoogleGenerativeAI (Google)
    llm = ChatGoogleGenerativeAI(model="gemma-3-27b-it", temperature=0.3)
    
    prompt_template = """
    Role: You are "Carmen" (‡∏Ñ‡∏≤‡∏£‡πå‡πÄ‡∏°‡∏ô), a professional and gentle AI Assistant for Carmen Software.
    
    Instructions:
    1. Answer the question based ONLY on the provided context.
    2. **Tone:** Be polite, helpful, and professional.
    3. **Language Rules:**
       - If the user asks in **Thai**: Answer in **Thai** and MUST use female polite particles (e.g., ‡∏Ñ‡πà‡∏∞, ‡∏Ñ‡∏∞, ‡∏ô‡∏∞‡∏Ñ‡∏∞).
       - If the user asks in **English** or explicitly requests English: Answer in **English**.
    
    Context Information:
    {context}
    
    User Question: {question}
    
    Answer:
    """
    PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
except Exception as e:
    print(f"‚ùå AI Init Error: {e}")
    vectorstore = None
    llm = None

# --- Setup FastAPI ---
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- üîê Login API ---
@app.post("/token")
async def login_for_access_token(form_data: OAuth2PasswordRequestForm = Depends(), db: Session = Depends(get_db)):
    user = db.query(UserModel).filter(UserModel.username == form_data.username).first()
    
    from passlib.context import CryptContext
    pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")
    
    if not user or not pwd_context.verify(form_data.password, user.hashed_password):
        raise HTTPException(
            status_code=401,
            detail="Incorrect username or password",
            headers={"WWW-Authenticate": "Bearer"},
        )
    
    access_token = create_access_token(data={"sub": user.username})
    return {"access_token": access_token, "token_type": "bearer", "client_namespace": user.client_id}

# --- üìú API ‡∏î‡∏∂‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡πÅ‡∏ä‡∏ó ---
@app.get("/chat/history")
async def get_chat_history(
    current_user: UserModel = Depends(get_current_user), 
    db: Session = Depends(get_db)
):
    history = db.query(ChatHistory).filter(ChatHistory.user_id == current_user.id)\
                .order_by(desc(ChatHistory.timestamp))\
                .limit(50).all()
    return history[::-1] 

# --- üìä API ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Admin ‡∏î‡∏π Log Token (PostgreSQL) ---
@app.get("/admin/logs")
async def get_token_logs(
    current_user: UserModel = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå Admin (‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏ñ‡πâ‡∏≤ client_id = global ‡∏Ñ‡∏∑‡∏≠ admin)
    if current_user.client_id != "global":
        raise HTTPException(status_code=403, detail="Admin access only")
    
    # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ï‡∏≤‡∏£‡∏≤‡∏á TokenLog
    logs = db.query(TokenLog).order_by(desc(TokenLog.timestamp)).limit(50).all()
    return logs

# --- üí¨ Chat API (Updated with Token Logging) ---
class Question(BaseModel):
    text: str

@app.post("/chat")
async def chat_endpoint(
    question: Question, 
    background_tasks: BackgroundTasks, # ‚úÖ ‡∏£‡∏±‡∏ö BackgroundTasks
    current_user: UserModel = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    if not vectorstore: raise HTTPException(status_code=500, detail="AI Not Ready")
    
    try:
        user_message = question.text
        
        # Save User Msg
        user_msg_db = ChatHistory(user_id=current_user.id, sender="user", message=user_message)
        db.add(user_msg_db)
        db.commit()
        
        # --- Logic AI ---
        client_ns = current_user.client_id 
        docs_private = []
        if client_ns and client_ns != "global":
            docs_private = vectorstore.similarity_search(user_message, k=2, namespace=client_ns)
        
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô Global Namespace ‡∏î‡πâ‡∏ß‡∏¢
        docs_common = vectorstore.similarity_search(user_message, k=2, namespace="global") 
        all_docs = docs_private + docs_common

        bot_ans = ""
        usage = {} # ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÄ‡∏Å‡πá‡∏ö Token

        if not all_docs:
            bot_ans = "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ñ‡πà‡∏∞"
        else:
            # ‚ö†Ô∏è ‡πÄ‡∏≠‡∏≤ StrOutputParser ‡∏≠‡∏≠‡∏Å ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ Object ‡πÄ‡∏ï‡πá‡∏°‡πÜ (‡∏ó‡∏µ‡πà‡∏°‡∏µ Usage Metadata)
            chain = PROMPT | llm 
            context_text = "\n\n".join([d.page_content for d in all_docs])
            
            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å AI
            response = chain.invoke({"context": context_text, "question": user_message})
            
            # ‡πÅ‡∏Å‡∏∞‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÅ‡∏•‡∏∞ Token
            bot_ans = response.content
            usage = response.usage_metadata or {} # ‡∏î‡∏∂‡∏á Token Info
            
            # ‚úÖ ‡∏™‡∏±‡πà‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Token ‡∏•‡∏á DB (‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏á)
            if usage:
                background_tasks.add_task(
                    log_token_usage,
                    namespace=client_ns,
                    model="gemma-3-27b-it",
                    input_tk=usage.get("input_tokens", 0),
                    output_tk=usage.get("output_tokens", 0),
                    query=user_message # (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏≤‡∏Å‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ‡πÉ‡∏´‡πâ‡πÅ‡∏Å‡πâ‡πÄ‡∏õ‡πá‡∏ô "")
                )

        # Save Bot Msg
        bot_msg_db = ChatHistory(user_id=current_user.id, sender="bot", message=bot_ans)
        db.add(bot_msg_db)
        db.commit() 
        db.refresh(bot_msg_db) 

        return {
            "answer": bot_ans, 
            "message_id": bot_msg_db.id,
            "usage_debug": usage # ‡∏™‡πà‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏î‡∏π‡πÄ‡∏•‡πà‡∏ô‡πÜ ‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö (Optional)
        }

    except Exception as e:
        print(f"Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# --- üëç Feedback API ---
class FeedbackRequest(BaseModel):
    score: int 

@app.post("/chat/feedback/{message_id}")
async def feedback_endpoint(
    message_id: int,
    feedback: FeedbackRequest,
    current_user: UserModel = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    msg = db.query(ChatHistory).filter(ChatHistory.id == message_id).first()
    
    if not msg:
        raise HTTPException(status_code=404, detail="Message not found")
        
    if msg.user_id != current_user.id:
        raise HTTPException(status_code=403, detail="Not your message")

    msg.feedback = feedback.score
    db.commit()
    return {"status": "success", "score": feedback.score}

# ==========================================
# üß† Training APIs (Manual, Upload, GitHub)
# ==========================================

# 1. Manual Input
class TrainingRequest(BaseModel):
    text: str
    namespace: str = "global"
    source: str = "admin_manual"

@app.post("/train")
async def train_data(
    request: TrainingRequest,
    current_user: UserModel = Depends(get_current_user), 
    db: Session = Depends(get_db)
):
    if current_user.client_id != "global" and request.namespace != current_user.client_id:
         raise HTTPException(status_code=403, detail="‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡∏™‡∏≠‡∏ô‡πÉ‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ")

    if not vectorstore:
        raise HTTPException(status_code=500, detail="‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Pinecone ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ")

    try:
        print(f"üß† Learning: {request.text[:50]}... -> Namespace: {request.namespace}")
        
        vectorstore.add_texts(
            texts=[request.text],
            metadatas=[{
                "source": request.source,
                "added_by": current_user.username,
                "timestamp": str(datetime.now())
            }],
            namespace=request.namespace
        )
        return {"status": "success", "message": "‡∏à‡∏≥‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏∞! üíæ"}

    except Exception as e:
        print(f"Training Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# 2. File Upload
@app.post("/train/upload")
async def train_upload(
    file: UploadFile = File(...),
    namespace: str = "global",
    source: str = "File Upload",
    current_user: UserModel = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    if current_user.client_id != "global" and namespace != current_user.client_id:
        raise HTTPException(status_code=403, detail="‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡∏™‡∏≠‡∏ô‡πÉ‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ")
    
    if not vectorstore:
        raise HTTPException(status_code=500, detail="‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Pinecone ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ")

    text_content = ""
    filename = file.filename.lower()

    try:
        contents = await file.read()
        
        if filename.endswith(".pdf"):
            from pypdf import PdfReader
            pdf_file = io.BytesIO(contents)
            reader = PdfReader(pdf_file)
            for page in reader.pages:
                text_content += page.extract_text() + "\n"
                
        elif filename.endswith(".csv"):
            df = pd.read_csv(io.BytesIO(contents))
            text_content = df.to_string(index=False)
            
        elif filename.endswith((".txt", ".md", ".py", ".js", ".html", ".css", ".json")):
            text_content = contents.decode("utf-8")
            
        else:
            raise HTTPException(status_code=400, detail="‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ PDF, CSV, ‡πÅ‡∏•‡∏∞ Text/Code Files ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô")

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        chunks = text_splitter.split_text(text_content)
        
        print(f"üìÑ File: {filename} -> {len(chunks)} Chunks")

        vectorstore.add_texts(
            texts=chunks,
            metadatas=[{
                "source": f"{source} ({filename})",
                "added_by": current_user.username,
                "timestamp": str(datetime.now())
            } for _ in chunks],
            namespace=namespace
        )

        return {"status": "success", "message": f"‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå {filename} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! ({len(chunks)} ‡∏™‡πà‡∏ß‡∏ô)"}

    except Exception as e:
        print(f"Upload Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ==========================================
# ‚öôÔ∏è Helper Functions for Training
# ==========================================

def get_modified_files(repo, days=30):
    """‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏ô X ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏°‡∏≤"""
    print(f"üïµÔ∏è‚Äç‚ôÇÔ∏è Checking for updates in the last {days} days...")
    since_date = datetime.now() - timedelta(days=days)
    
    modified_files = set()
    try:
        commits = repo.get_commits(since=since_date)
        for commit in commits:
            for file in commit.files:
                if file.filename.endswith((".md", ".mdx", ".txt", ".csv", ".py", ".js", ".ts", ".html", ".css", ".json")):
                    modified_files.add(file.filename)
                    
        print(f"   ‚ú® Found {len(modified_files)} modified files.")
        return list(modified_files)
    except Exception as e:
        print(f"   ‚ùå Error getting commits: {e}")
        return []

def get_file_content(repo, file_path):
    """‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (‡∏£‡∏∞‡∏ö‡∏∏ Path)"""
    try:
        file_content = repo.get_contents(file_path)
        return Document(
            page_content=file_content.decoded_content.decode("utf-8"),
            metadata={"source": file_content.html_url, "file_path": file_path}
        )
    except Exception as e:
        print(f"   ‚ö†Ô∏è Error reading {file_path}: {e}")
        return None

def get_github_docs(repo_name, access_token):
    print(f"üïµÔ∏è‚Äç‚ôÇÔ∏è Connecting to GitHub Repo: '{repo_name}'")
    repo_name = repo_name.strip()
    access_token = access_token.strip() if access_token else None
    
    docs = []
    try:
        if access_token:
            print("   üîë Using Access Token")
            g = Github(access_token)
        else:
            print("   üåê Using Anonymous Access (Public Repo Only)")
            g = Github()

        repo = g.get_repo(repo_name)
        print(f"   ‚úÖ Found Repo: {repo.full_name} (Default Branch: {repo.default_branch})")

        contents = repo.get_contents("")
        file_count = 0
        
        while contents:
            file_content = contents.pop(0)
            if file_content.type == "dir":
                contents.extend(repo.get_contents(file_content.path))
            else:
                ALLOWED_EXTENSIONS = (
                    ".md", ".mdx", ".txt", ".csv", 
                    ".py", ".js", ".ts", ".html", ".css", ".json"
                )
                
                if file_content.path.endswith(ALLOWED_EXTENSIONS):
                    file_count += 1
                    try:
                        decoded_content = file_content.decoded_content.decode("utf-8")
                        docs.append(Document(
                            page_content=decoded_content,
                            metadata={
                                "source": file_content.html_url,
                                "file_path": file_content.path
                            }
                        ))
                    except Exception as decode_err:
                        print(f"     ‚ö†Ô∏è Skip {file_content.path}: {decode_err}")

        print(f"   üìä Summary: Found {file_count} valid files in repo.")
        return docs

    except Exception as e:
        print(f"‚ùå GitHub Error Detail: {type(e).__name__} - {str(e)}")
        return []
    
training_state = {
    "is_running": False,
    "progress": 0,
    "total_chunks": 0,
    "processed_chunks": 0,
    "status": "Idle",
    "logs": [],
    "start_time": 0,
    "estimated_remaining": 0,
    "abort": False
}    
    
def add_log(message: str):
    print(message)
    timestamp = datetime.now().strftime("%H:%M:%S")
    training_state["logs"].append(f"[{timestamp}] {message}")
    if len(training_state["logs"]) > 20:
        training_state["logs"].pop(0)    

def process_url_training(url: str, namespace: str, user_name: str, recursive: bool = False, depth: int = 2):
    global training_state
    
    training_state.update({
        "is_running": True, "progress": 0, "total_chunks": 0, "processed_chunks": 0,
        "status": "Starting", "logs": [], "start_time": time.time(), "estimated_remaining": 0, "abort": False 
    })

    try:
        add_log(f"üåê ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠: {url}")
        docs = []
        
        if recursive:
            add_log(f"üï∑Ô∏è Mode: Recursive Crawling (Depth: {depth})")
            add_log("‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÑ‡∏ï‡πà‡∏•‡∏¥‡∏á‡∏Å‡πå... ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏™‡∏±‡∏Å‡∏û‡∏±‡∏Å")
            loader = RecursiveUrlLoader(
                url=url, max_depth=depth, extractor=lambda x: Soup(x, "html.parser").text, prevent_outside=True
            )
            docs = loader.load()
            add_log(f"‚úÖ ‡πÄ‡∏à‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(docs)} ‡∏´‡∏ô‡πâ‡∏≤")
            # Log links
            add_log("üìã ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ URL ‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏û‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î:")
            for i, doc in enumerate(docs):
                url_found = doc.metadata.get("source", "Unknown URL")
                add_log(f"   üëâ {i+1}. {url_found}")
            add_log(f"-----------------------------------------------------")

        else: 
            add_log("üìÑ Mode: Single Page (‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ)")
            loader = WebBaseLoader(url)
            docs = loader.load()

        if not docs:
            add_log("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏ö‡∏≠‡∏ó")
            training_state["status"] = "Failed"
            training_state["is_running"] = False
            return

        add_log(f"‚úÇÔ∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡πÅ‡∏•‡∏∞‡∏´‡∏±‡πà‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å {len(docs)} ‡∏´‡∏ô‡πâ‡∏≤...")
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        chunks = text_splitter.split_documents(docs)
        
        total_chunks = len(chunks)
        training_state["total_chunks"] = total_chunks
        add_log(f"üì¶ ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {total_chunks} ‡∏ä‡∏¥‡πâ‡∏ô (Chunks)")

        for chunk in chunks:
            chunk.metadata["added_by"] = user_name
            chunk.metadata["timestamp"] = str(datetime.now())
            chunk.metadata["source_type"] = "web_url"
            if "source" not in chunk.metadata: chunk.metadata["source"] = url

        batch_size = 30
        sleep_time = 20
        
        for i in range(0, total_chunks, batch_size):
            if training_state["abort"]:
                add_log("‚õî ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ñ‡∏π‡∏Å‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å")
                training_state["status"] = "Cancelled"
                training_state["is_running"] = False
                return

            current_time = time.time()
            elapsed_time = current_time - training_state["start_time"]
            processed = i
            if processed > 0:
                speed = processed / elapsed_time
                remaining_chunks = total_chunks - processed
                eta = remaining_chunks / speed if speed > 0 else 0
                training_state["estimated_remaining"] = int(eta)
            
            percent = int((i / total_chunks) * 100)
            training_state["progress"] = percent
            training_state["status"] = "Processing"
            add_log(f"üì§ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡πà‡∏á Batch {(i//batch_size)+1} (Process: {i}/{total_chunks})")

            batch = chunks[i : i + batch_size]
            vectorstore.add_documents(documents=batch, namespace=namespace)
            
            add_log(f"‚úÖ Batch {(i//batch_size)+1} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! ‡∏û‡∏±‡∏Å {sleep_time} ‡∏ß‡∏¥...")
            for _ in range(sleep_time):
                if training_state["abort"]: break
                time.sleep(1)

        training_state["progress"] = 100
        training_state["status"] = "Completed"
        training_state["is_running"] = False
        add_log("üéâ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå! ‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")

    except Exception as e:
        training_state["status"] = "Error"
        training_state["is_running"] = False
        add_log(f"‚ö†Ô∏è Error: {str(e)}")

def process_drive_training(folder_id: str, key_path: str, namespace: str, user_name: str):
    global training_state
    
    training_state.update({
        "is_running": True, "progress": 0, "total_chunks": 0, "processed_chunks": 0,
        "status": "Starting", "logs": [], "start_time": time.time(), "estimated_remaining": 0, "abort": False 
    })

    try:
        add_log(f"üìÅ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Google Drive (Folder ID: {folder_id})")
        
        creds = service_account.Credentials.from_service_account_file(
            key_path, scopes=['https://www.googleapis.com/auth/drive.readonly']
        )
        service = build('drive', 'v3', credentials=creds)

        all_items = []
        folders_stack = [folder_id]
        
        add_log("üï∑Ô∏è ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å Folder ‡∏¢‡πà‡∏≠‡∏¢...")
        
        while folders_stack:
            if training_state["abort"]: break
            current_folder = folders_stack.pop()
            
            try:
                results = service.files().list(
                    q=f"'{current_folder}' in parents and trashed=false",
                    fields="files(id, name, mimeType)",
                    pageSize=1000
                ).execute()
                items = results.get('files', [])
                
                for item in items:
                    if item['mimeType'] == 'application/vnd.google-apps.folder':
                        folders_stack.append(item['id'])
                    else:
                        all_items.append(item)
            except Exception as e:
                add_log(f"‚ö†Ô∏è ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á Folder {current_folder} ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {e}")

        add_log(f"‚úÖ ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(all_items)} ‡πÑ‡∏ü‡∏•‡πå (‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡∏ä‡∏±‡πâ‡∏ô)")

        docs = []
        for idx, item in enumerate(all_items):
            if training_state["abort"]: break
            file_id = item['id']
            name = item['name']
            mime = item['mimeType']
            content = ""

            try:
                if mime == 'application/vnd.google-apps.document':
                    add_log(f"   [{idx+1}/{len(all_items)}] üîÑ ‡πÅ‡∏õ‡∏•‡∏á G-Doc: {name}")
                    request = service.files().export_media(fileId=file_id, mimeType='text/plain')
                    content = request.execute().decode('utf-8')

                elif name.endswith(('.md', '.txt', '.json', '.py', '.js', '.csv')) or mime.startswith('text/'):
                    add_log(f"   [{idx+1}/{len(all_items)}] ‚¨áÔ∏è ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå: {name}")
                    request = service.files().get_media(fileId=file_id)
                    fh = io.BytesIO()
                    downloader = MediaIoBaseDownload(fh, request)
                    done = False
                    while done is False:
                        status, done = downloader.next_chunk()
                    fh.seek(0)
                    content = fh.read().decode('utf-8', errors='ignore')
                else:
                    continue

                if content.strip():
                    doc = Document(
                        page_content=content,
                        metadata={"source": name, "title": name, "file_id": file_id}
                    )
                    docs.append(doc)

            except Exception as e:
                add_log(f"   ‚ùå Error {name}: {str(e)}")

        if not docs:
            add_log("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢")
            training_state["status"] = "Failed"
            training_state["is_running"] = False
            return

        add_log(f"‚úÖ ‡πÑ‡∏î‡πâ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏ó‡∏£‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(docs)} ‡∏â‡∏ö‡∏±‡∏ö")
        add_log(f"‚úÇÔ∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏´‡∏±‡πà‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤...")
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        chunks = text_splitter.split_documents(docs)
        
        total_chunks = len(chunks)
        training_state["total_chunks"] = total_chunks
        add_log(f"üì¶ ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {total_chunks} ‡∏ä‡∏¥‡πâ‡∏ô")

        for chunk in chunks:
            chunk.metadata["added_by"] = user_name
            chunk.metadata["timestamp"] = str(datetime.now())
            chunk.metadata["source_type"] = "google_drive"
            chunk.metadata["folder_id"] = folder_id

        batch_size = 30
        sleep_time = 20
        
        for i in range(0, total_chunks, batch_size):
            if training_state["abort"]:
                add_log("‚õî ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ñ‡∏π‡∏Å‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å")
                training_state["status"] = "Cancelled"
                training_state["is_running"] = False
                return

            current_time = time.time()
            elapsed_time = current_time - training_state["start_time"]
            processed = i
            if processed > 0:
                speed = processed / elapsed_time
                remaining_chunks = total_chunks - processed
                eta = remaining_chunks / speed if speed > 0 else 0
                training_state["estimated_remaining"] = int(eta)
            
            percent = int((i / total_chunks) * 100)
            training_state["progress"] = percent
            training_state["status"] = "Processing"
            add_log(f"üì§ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡πà‡∏á Batch {(i//batch_size)+1} (Process: {i}/{total_chunks})")

            batch = chunks[i : i + batch_size]
            vectorstore.add_documents(documents=batch, namespace=namespace)
            
            add_log(f"‚úÖ Batch {(i//batch_size)+1} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! ‡∏û‡∏±‡∏Å {sleep_time} ‡∏ß‡∏¥...")
            for _ in range(sleep_time):
                if training_state["abort"]: break
                time.sleep(1)

        training_state["progress"] = 100
        training_state["status"] = "Completed"
        training_state["is_running"] = False
        add_log("üéâ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå! Google Drive ‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")

    except Exception as e:
        training_state["status"] = "Error"
        training_state["is_running"] = False
        add_log(f"‚ö†Ô∏è Error: {str(e)}")  

def process_github_training(repo_name: str, token: str, namespace: str, user_name: str, incremental: bool = False):
    global training_state
    
    training_state.update({
        "is_running": True, "progress": 0, "total_chunks": 0, "processed_chunks": 0,
        "status": "Starting", "logs": [], "start_time": time.time(), "estimated_remaining": 0, "abort": False
    })

    try:
        add_log(f"üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ GitHub Repo: {repo_name}")
        mode_text = "Incremental Update" if incremental else "Full Load"
        add_log(f"üîÑ Mode: {mode_text}")

        if token: g = Github(token)
        else: g = Github()
        repo = g.get_repo(repo_name)

        docs = []
        if incremental:
            file_paths = get_modified_files(repo, days=30)
            if not file_paths:
                add_log("‚úÖ ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÉ‡∏´‡∏°‡πà‡πÜ ‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á 30 ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏°‡∏≤")
                training_state["status"] = "Completed"
                training_state["progress"] = 100
                training_state["is_running"] = False
                return
            
            add_log(f"üì• ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î {len(file_paths)} ‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà...")
            for path in file_paths:
                if training_state["abort"]:
                    add_log("‚õî ‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡∏Å‡∏≤‡∏£‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î")
                    training_state["status"] = "Cancelled"
                    training_state["is_running"] = False
                    return
                doc = get_file_content(repo, path)
                if doc: docs.append(doc)
        else:
            docs = get_github_docs(repo_name, token)

        if not docs:
            add_log("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•")
            training_state["status"] = "Failed"
            training_state["is_running"] = False
            return

        add_log(f"‚úÇÔ∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏´‡∏±‡πà‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å {len(docs)} ‡πÑ‡∏ü‡∏•‡πå...")
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        chunks = text_splitter.split_documents(docs)
        
        total_chunks = len(chunks)
        training_state["total_chunks"] = total_chunks
        add_log(f"üì¶ ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {total_chunks} ‡∏ä‡∏¥‡πâ‡∏ô (Chunks)")

        for chunk in chunks:
            chunk.metadata["added_by"] = user_name
            chunk.metadata["timestamp"] = str(datetime.now())
            chunk.metadata["source_type"] = "github_repo"

        batch_size = 30  
        sleep_time = 20  
        
        for i in range(0, total_chunks, batch_size):
            if training_state["abort"]:
                add_log("‚õî ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ñ‡∏π‡∏Å‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡πÇ‡∏î‡∏¢ Admin")
                training_state["status"] = "Cancelled"
                training_state["is_running"] = False
                return 
            
            current_time = time.time()
            elapsed_time = current_time - training_state["start_time"]
            processed = i
            if processed > 0:
                speed = processed / elapsed_time
                remaining_chunks = total_chunks - processed
                eta = remaining_chunks / speed if speed > 0 else 0
                training_state["estimated_remaining"] = int(eta)
            
            percent = int((i / total_chunks) * 100)
            training_state["progress"] = percent
            training_state["processed_chunks"] = i
            training_state["status"] = "Processing"
            
            add_log(f"üì§ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡πà‡∏á Batch {(i//batch_size)+1} (Process: {i}/{total_chunks}) - ETA: {int(training_state['estimated_remaining'])}s")

            batch = chunks[i : i + batch_size]
            vectorstore.add_documents(documents=batch, namespace=namespace)
            
            add_log(f"‚úÖ Batch {(i//batch_size)+1} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! ‡∏û‡∏±‡∏Å‡∏´‡∏≤‡∏¢‡πÉ‡∏à {sleep_time} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ...")
            
            for _ in range(sleep_time):
                if training_state["abort"]: break
                time.sleep(1)
            
        training_state["progress"] = 100
        training_state["status"] = "Completed"
        training_state["is_running"] = False
        add_log(f"üéâ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå! ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {total_chunks} ‡∏ä‡∏¥‡πâ‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")
        
    except Exception as e:
        training_state["status"] = "Error"
        training_state["is_running"] = False
        add_log(f"‚ö†Ô∏è Error: {str(e)}")

@app.post("/train/cancel")
async def cancel_training(current_user: UserModel = Depends(get_current_user)):
    global training_state
    if training_state["is_running"]:
        training_state["abort"] = True
        training_state["status"] = "Cancelling..."
        add_log("üõë ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å! ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£...")
    return {"status": "success", "message": "Cancellation requested"}        

@app.get("/train/status")
async def get_training_status():
    return training_state

class GithubRequest(BaseModel):
    repo_name: str
    github_token: str
    namespace: str = "global"
    incremental: bool = False 

@app.post("/train/github")
async def train_github(
    request: GithubRequest,
    background_tasks: BackgroundTasks,
    current_user: UserModel = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    if current_user.client_id != "global" and request.namespace != current_user.client_id:
         raise HTTPException(status_code=403, detail="‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå")

    background_tasks.add_task(
        process_github_training, 
        request.repo_name, 
        request.github_token, 
        request.namespace, 
        current_user.username,
        request.incremental
    )
    
    mode_text = "Incremental Update" if request.incremental else "Full Load"
    return {"status": "success", "message": f"‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ {mode_text} ‡πÅ‡∏•‡πâ‡∏ß!"}

class UrlRequest(BaseModel):
    url: str
    namespace: str = "global"
    recursive: bool = False
    depth: int = 2

@app.post("/train/url")
async def train_url(
    request: UrlRequest,
    background_tasks: BackgroundTasks,
    current_user: UserModel = Depends(get_current_user)
):
    if current_user.client_id != "global" and request.namespace != current_user.client_id:
         raise HTTPException(status_code=403, detail="‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå")

    background_tasks.add_task(
        process_url_training, 
        request.url, 
        request.namespace, 
        current_user.username,
        request.recursive,
        request.depth
    )
    return {"status": "success", "message": "Start processing URL"}

@app.post("/train/drive")
async def train_drive(
    background_tasks: BackgroundTasks,  # ‚úÖ BackgroundTasks ‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏¢‡∏π‡πà‡∏Å‡πà‡∏≠‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ default
    folder_id: str = Form(...),
    namespace: str = Form(...),
    file: UploadFile = File(...), 
    current_user: UserModel = Depends(get_current_user)
):
    if current_user.client_id != "global" and namespace != current_user.client_id:
         raise HTTPException(status_code=403, detail="‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå")

    key_filename = f"service_key_{current_user.username}.json"
    with open(key_filename, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    
    background_tasks.add_task(
        process_drive_training, 
        folder_id, 
        key_filename, 
        namespace, 
        current_user.username
    )
    
    return {"status": "success", "message": "Start processing Google Drive"}

# --- üõ†Ô∏è Debug / Reset DB API ---
@app.get("/debug/init-db")
async def init_database_endpoint(db: Session = Depends(get_db)):
    try:
        print("üöÄ Resetting Database via API...")
        Base.metadata.drop_all(bind=engine)
        Base.metadata.create_all(bind=engine)

        users_data = [
            ("manager_seaside", "1234", "hotel-seaside"),
            ("manager_city", "1234", "hotel-city"),
            ("admin", "admin", "global")
        ]
        
        created_users = []
        for username, pwd, ns in users_data:
            new_user = UserModel(
                username=username,
                hashed_password=get_password_hash(pwd),
                client_id=ns,
                full_name=username 
            )
            db.add(new_user)
            created_users.append(username)
        
        db.commit()
        return {
            "status": "success", 
            "message": "üéâ Database Reset & Initialized Successfully!", 
            "users_created": created_users
        }
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return {"status": "error", "message": str(e)}

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8000))
    uvicorn.run(app, host="0.0.0.0", port=port)