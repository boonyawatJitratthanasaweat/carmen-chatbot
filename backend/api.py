import time
from langchain_community.document_loaders import GoogleDriveLoader # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ
import shutil # ‡πÄ‡∏≠‡∏≤‡πÑ‡∏ß‡πâ‡πÄ‡∏ã‡∏ü‡πÑ‡∏ü‡∏•‡πå JSON ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
import os
from datetime import datetime, timedelta 
from fastapi import FastAPI, HTTPException, BackgroundTasks, Depends, Request, Form, File, UploadFile
from fastapi.security import OAuth2PasswordRequestForm
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from sqlalchemy.orm import Session
from sqlalchemy import desc
import os
import uvicorn
import io
import pandas as pd
from pathlib import Path

from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload
import io

# AI & LangChain
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_pinecone import PineconeVectorStore
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from langchain.schema import Document
from github import Github

from dotenv import load_dotenv

from langchain_community.document_loaders import WebBaseLoader # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ
import validators # (Optional: ‡πÑ‡∏ß‡πâ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤ URL ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏´‡∏° ‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏£)

# ‡πÄ‡∏û‡∏¥‡πà‡∏° RecursiveUrlLoader ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤
from langchain_community.document_loaders import RecursiveUrlLoader
from bs4 import BeautifulSoup as Soup 

# Import ‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏∞‡∏ö‡∏ö
from .database import Base, engine
from .auth import get_db, create_access_token, get_current_user, get_password_hash, User as UserModel, ChatHistory

# ‡πÇ‡∏´‡∏•‡∏î ENV
env_path = Path(__file__).parent / '.env'
load_dotenv(dotenv_path=env_path)

# --- ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÉ‡∏ô Database ---
Base.metadata.create_all(bind=engine)

# --- Config ---
INDEX_NAME = os.environ.get("PINECONE_INDEX_NAME", "docscarmencloud")

# --- ‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏°‡∏≠‡∏á AI ---
print("üß† Loading AI Brain...")
try:
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
    vectorstore = PineconeVectorStore(index_name=INDEX_NAME, embedding=embeddings)
    llm = ChatGoogleGenerativeAI(model="gemma-3-27b-it", temperature=0.3)
    
    prompt_template = """
    Role: You are "Carmen" (‡∏Ñ‡∏≤‡∏£‡πå‡πÄ‡∏°‡∏ô), a professional and gentle AI Assistant for Carmen Software.
    
    Instructions:
    1. Answer the question based ONLY on the provided context.
    2. **Tone:** Be polite, helpful, and professional.
    3. **Language Rules:**
       - If the user asks in **Thai**: Answer in **Thai** and MUST use female polite particles (e.g., ‡∏Ñ‡πà‡∏∞, ‡∏Ñ‡∏∞, ‡∏ô‡∏∞‡∏Ñ‡∏∞).
       - If the user asks in **English** or explicitly requests English: Answer in **English**.
    
    Context Information:
    {context}
    
    User Question: {question}
    
    Answer:
    """
    PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
except Exception as e:
    print(f"‚ùå AI Init Error: {e}")
    vectorstore = None
    llm = None

# --- Setup FastAPI ---
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- üîê Login API ---
@app.post("/token")
async def login_for_access_token(form_data: OAuth2PasswordRequestForm = Depends(), db: Session = Depends(get_db)):
    user = db.query(UserModel).filter(UserModel.username == form_data.username).first()
    
    from passlib.context import CryptContext
    pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")
    
    if not user or not pwd_context.verify(form_data.password, user.hashed_password):
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect username or password",
            headers={"WWW-Authenticate": "Bearer"},
        )
    
    access_token = create_access_token(data={"sub": user.username})
    return {"access_token": access_token, "token_type": "bearer", "client_namespace": user.client_id}

# --- üìú API ‡∏î‡∏∂‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡πÅ‡∏ä‡∏ó ---
@app.get("/chat/history")
async def get_chat_history(
    current_user: UserModel = Depends(get_current_user), 
    db: Session = Depends(get_db)
):
    history = db.query(ChatHistory).filter(ChatHistory.user_id == current_user.id)\
                .order_by(desc(ChatHistory.timestamp))\
                .limit(50).all()
    return history[::-1] 

# --- üí¨ Chat API ---
class Question(BaseModel):
    text: str

@app.post("/chat")
async def chat_endpoint(
    question: Question, 
    current_user: UserModel = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    if not vectorstore: raise HTTPException(status_code=500, detail="AI Not Ready")
    
    try:
        user_message = question.text
        
        # Save User Msg
        user_msg_db = ChatHistory(user_id=current_user.id, sender="user", message=user_message)
        db.add(user_msg_db)
        db.commit()
        
        # --- Logic AI ---
        client_ns = current_user.client_id 
        docs_private = []
        if client_ns and client_ns != "global":
            docs_private = vectorstore.similarity_search(user_message, k=2, namespace=client_ns)
        
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô Global Namespace ‡∏î‡πâ‡∏ß‡∏¢
        docs_common = vectorstore.similarity_search(user_message, k=2, namespace="global") 
        all_docs = docs_private + docs_common

        if not all_docs:
            bot_ans = "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ñ‡πà‡∏∞"
        else:
            chain = PROMPT | llm | StrOutputParser()
            context_text = "\n\n".join([d.page_content for d in all_docs])
            bot_ans = chain.invoke({"context": context_text, "question": user_message})

        # Save Bot Msg
        bot_msg_db = ChatHistory(user_id=current_user.id, sender="bot", message=bot_ans)
        db.add(bot_msg_db)
        db.commit() 
        db.refresh(bot_msg_db) 

        return {
            "answer": bot_ans, 
            "message_id": bot_msg_db.id 
        }

    except Exception as e:
        print(f"Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# --- üëç Feedback API ---
class FeedbackRequest(BaseModel):
    score: int 

@app.post("/chat/feedback/{message_id}")
async def feedback_endpoint(
    message_id: int,
    feedback: FeedbackRequest,
    current_user: UserModel = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    msg = db.query(ChatHistory).filter(ChatHistory.id == message_id).first()
    
    if not msg:
        raise HTTPException(status_code=404, detail="Message not found")
        
    if msg.user_id != current_user.id:
        raise HTTPException(status_code=403, detail="Not your message")

    msg.feedback = feedback.score
    db.commit()
    return {"status": "success", "score": feedback.score}

# ==========================================
# üß† Training APIs (Manual, Upload, GitHub)
# ==========================================

# 1. Manual Input
class TrainingRequest(BaseModel):
    text: str
    namespace: str = "global"  # ‚úÖ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô default ‡πÄ‡∏õ‡πá‡∏ô global
    source: str = "admin_manual"

@app.post("/train")
async def train_data(
    request: TrainingRequest,
    current_user: UserModel = Depends(get_current_user), 
    db: Session = Depends(get_db)
):
    if current_user.client_id != "global" and request.namespace != current_user.client_id:
         raise HTTPException(status_code=403, detail="‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡∏™‡∏≠‡∏ô‡πÉ‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ")

    if not vectorstore:
        raise HTTPException(status_code=500, detail="‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Pinecone ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ")

    try:
        print(f"üß† Learning: {request.text[:50]}... -> Namespace: {request.namespace}")
        
        vectorstore.add_texts(
            texts=[request.text],
            metadatas=[{
                "source": request.source,
                "added_by": current_user.username,
                "timestamp": str(datetime.now())
            }],
            namespace=request.namespace
        )
        return {"status": "success", "message": "‡∏à‡∏≥‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏∞! üíæ"}

    except Exception as e:
        print(f"Training Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# 2. File Upload
@app.post("/train/upload")
async def train_upload(
    file: UploadFile = File(...),
    namespace: str = "global", # ‚úÖ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô default ‡πÄ‡∏õ‡πá‡∏ô global
    source: str = "File Upload",
    current_user: UserModel = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    if current_user.client_id != "global" and namespace != current_user.client_id:
        raise HTTPException(status_code=403, detail="‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡∏™‡∏≠‡∏ô‡πÉ‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ")
    
    if not vectorstore:
        raise HTTPException(status_code=500, detail="‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Pinecone ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ")

    text_content = ""
    filename = file.filename.lower()

    try:
        contents = await file.read()
        
        if filename.endswith(".pdf"):
            from pypdf import PdfReader
            pdf_file = io.BytesIO(contents)
            reader = PdfReader(pdf_file)
            for page in reader.pages:
                text_content += page.extract_text() + "\n"
                
        elif filename.endswith(".csv"):
            df = pd.read_csv(io.BytesIO(contents))
            text_content = df.to_string(index=False)
            
        elif filename.endswith((".txt", ".md", ".py", ".js", ".html", ".css", ".json")):
            text_content = contents.decode("utf-8")
            
        else:
            raise HTTPException(status_code=400, detail="‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ PDF, CSV, ‡πÅ‡∏•‡∏∞ Text/Code Files ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô")

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        chunks = text_splitter.split_text(text_content)
        
        print(f"üìÑ File: {filename} -> {len(chunks)} Chunks")

        vectorstore.add_texts(
            texts=chunks,
            metadatas=[{
                "source": f"{source} ({filename})",
                "added_by": current_user.username,
                "timestamp": str(datetime.now())
            } for _ in chunks],
            namespace=namespace
        )

        return {"status": "success", "message": f"‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå {filename} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! ({len(chunks)} ‡∏™‡πà‡∏ß‡∏ô)"}

    except Exception as e:
        print(f"Upload Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# 3. GitHub Logic

def get_modified_files(repo, days=30):
    """‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏ô X ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏°‡∏≤"""
    print(f"üïµÔ∏è‚Äç‚ôÇÔ∏è Checking for updates in the last {days} days...")
    since_date = datetime.now() - timedelta(days=days)
    
    modified_files = set()
    try:
        # ‡∏î‡∏∂‡∏á Commit ‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á
        commits = repo.get_commits(since=since_date)
        
        for commit in commits:
            for file in commit.files:
                # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö
                if file.filename.endswith((".md", ".mdx", ".txt", ".csv", ".py", ".js", ".ts", ".html", ".css", ".json")):
                    modified_files.add(file.filename)
                    
        print(f"   ‚ú® Found {len(modified_files)} modified files.")
        return list(modified_files)
    except Exception as e:
        print(f"   ‚ùå Error getting commits: {e}")
        return []

def get_file_content(repo, file_path):
    """‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (‡∏£‡∏∞‡∏ö‡∏∏ Path)"""
    try:
        file_content = repo.get_contents(file_path)
        return Document(
            page_content=file_content.decoded_content.decode("utf-8"),
            metadata={"source": file_content.html_url, "file_path": file_path}
        )
    except Exception as e:
        print(f"   ‚ö†Ô∏è Error reading {file_path}: {e}")
        return None

def get_github_docs(repo_name, access_token):
    print(f"üïµÔ∏è‚Äç‚ôÇÔ∏è Connecting to GitHub Repo: '{repo_name}'")
    
    # Clean Inputs
    repo_name = repo_name.strip()
    access_token = access_token.strip() if access_token else None
    
    docs = []
    try:
        # 1. ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ GitHub
        if access_token:
            print("   üîë Using Access Token")
            g = Github(access_token)
        else:
            print("   üåê Using Anonymous Access (Public Repo Only)")
            g = Github()

        # 2. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Repo
        repo = g.get_repo(repo_name)
        print(f"   ‚úÖ Found Repo: {repo.full_name} (Default Branch: {repo.default_branch})")

        # 3. ‡∏î‡∏∂‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (Recursive)
        contents = repo.get_contents("")
        file_count = 0
        
        while contents:
            file_content = contents.pop(0)
            
            if file_content.type == "dir":
                contents.extend(repo.get_contents(file_content.path))
            else:
                # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö (Code, Text, Config)
                ALLOWED_EXTENSIONS = (
                    ".md", ".mdx", ".txt", ".csv", 
                    ".py", ".js", ".ts", ".html", ".css", ".json"
                )
                
                if file_content.path.endswith(ALLOWED_EXTENSIONS):
                    file_count += 1
                    try:
                        # Decode ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå
                        decoded_content = file_content.decoded_content.decode("utf-8")
                        
                        docs.append(Document(
                            page_content=decoded_content,
                            metadata={
                                "source": file_content.html_url,
                                "file_path": file_content.path
                            }
                        ))
                        # print(f"     üìÑ Loaded: {file_content.path}") # ‡∏õ‡∏¥‡∏î‡πÑ‡∏ß‡πâ‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÑ‡∏°‡πà‡∏£‡∏Å Log
                    except Exception as decode_err:
                        print(f"     ‚ö†Ô∏è Skip {file_content.path}: {decode_err}")

        print(f"   üìä Summary: Found {file_count} valid files in repo.")
        return docs

    except Exception as e:
        # üö® ‡πÅ‡∏à‡πâ‡∏á Error ‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
        print(f"‚ùå GitHub Error Detail: {type(e).__name__} - {str(e)}")
        
        # ‡∏Å‡∏£‡∏ì‡∏µ 404 (‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠)
        if "404" in str(e):
             print("   üëâ ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥: ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ä‡∏∑‡πà‡∏≠ Repo ‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å ‡∏´‡∏£‡∏∑‡∏≠‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô Private Repo ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏™‡πà Token")
        
        # ‡∏Å‡∏£‡∏ì‡∏µ 401 (‡∏£‡∏´‡∏±‡∏™‡∏ú‡∏¥‡∏î)
        if "401" in str(e) or "Bad credentials" in str(e):
             print("   üëâ ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥: Token ‡∏ú‡∏¥‡∏î ‡∏´‡∏£‡∏∑‡∏≠‡∏´‡∏°‡∏î‡∏≠‡∏≤‡∏¢‡∏∏")

        return []
    
training_state = {
    "is_running": False,
    "progress": 0,          # %
    "total_chunks": 0,
    "processed_chunks": 0,
    "status": "Idle",
    "logs": [],             # ‡πÄ‡∏Å‡πá‡∏ö Log ‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á 10 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î
    "start_time": 0,
    "estimated_remaining": 0,# ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
    "abort": False
}    
    
def add_log(message: str):
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏Å‡πá‡∏ö Log ‡πÅ‡∏•‡∏∞ Print ‡∏•‡∏á Console"""
    print(message)
    timestamp = datetime.now().strftime("%H:%M:%S")
    training_state["logs"].append(f"[{timestamp}] {message}")
    # ‡πÄ‡∏Å‡πá‡∏ö‡πÅ‡∏Ñ‡πà 20 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏û‡∏≠ (‡πÄ‡∏î‡∏µ‡πã‡∏¢‡∏ß Memory ‡πÄ‡∏ï‡πá‡∏°)
    if len(training_state["logs"]) > 20:
        training_state["logs"].pop(0)    

def process_url_training(url: str, namespace: str, user_name: str, recursive: bool = False, depth: int = 2):
    global training_state
    
    # Reset State
    training_state.update({
        "is_running": True,
        "progress": 0,
        "total_chunks": 0,
        "processed_chunks": 0,
        "status": "Starting",
        "logs": [],
        "start_time": time.time(),
        "estimated_remaining": 0,
        "abort": False 
    })

    try:
        add_log(f"üåê ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠: {url}")
        
        docs = []
        
        # ==========================================
        # ‚ö†Ô∏è ‡∏à‡∏∏‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ñ‡∏∑‡∏≠‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡∏Ñ‡∏£‡∏±‡∏ö (if / else)
        # ==========================================
        if recursive:
            add_log(f"üï∑Ô∏è Mode: Recursive Crawling (Depth: {depth})")
            add_log("‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÑ‡∏ï‡πà‡∏•‡∏¥‡∏á‡∏Å‡πå... ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏™‡∏±‡∏Å‡∏û‡∏±‡∏Å")
            
            loader = RecursiveUrlLoader(
                url=url, 
                max_depth=depth,
                extractor=lambda x: Soup(x, "html.parser").text,
                prevent_outside=True
            )
            docs = loader.load()
            add_log(f"‚úÖ ‡πÄ‡∏à‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(docs)} ‡∏´‡∏ô‡πâ‡∏≤")

            # --- ‡πÇ‡∏ä‡∏ß‡πå Log ‡∏•‡∏¥‡∏á‡∏Å‡πå ---
            add_log("üìã ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ URL ‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏û‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î:")
            for i, doc in enumerate(docs):
                url_found = doc.metadata.get("source", "Unknown URL")
                title_found = doc.metadata.get("title", "").strip()[:50]
                if title_found:
                    add_log(f"   üëâ {i+1}. {url_found} ({title_found}...)")
                else:
                    add_log(f"   üëâ {i+1}. {url_found}")
            add_log(f"-----------------------------------------------------")
            # ---------------------

        else: 
            # ‚ö†Ô∏è ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ else ‡πÅ‡∏•‡∏∞‡∏¢‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö if ‡∏Ç‡πâ‡∏≤‡∏á‡∏ö‡∏ô‡πÄ‡∏õ‡πä‡∏∞‡πÜ
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏™‡πà else ‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡∏ú‡∏¥‡∏î ‡∏°‡∏±‡∏ô‡∏à‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏Ñ‡∏π‡πà ‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á
            add_log("üìÑ Mode: Single Page (‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ)")
            loader = WebBaseLoader(url)
            docs = loader.load()

        # ==========================================

        if not docs:
            add_log("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏ö‡∏≠‡∏ó")
            training_state["status"] = "Failed"
            training_state["is_running"] = False
            return

        # 2. ‡∏´‡∏±‡πà‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Splitting)
        add_log(f"‚úÇÔ∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡πÅ‡∏•‡∏∞‡∏´‡∏±‡πà‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å {len(docs)} ‡∏´‡∏ô‡πâ‡∏≤...")
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        chunks = text_splitter.split_documents(docs)
        
        total_chunks = len(chunks)
        training_state["total_chunks"] = total_chunks
        add_log(f"üì¶ ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {total_chunks} ‡∏ä‡∏¥‡πâ‡∏ô (Chunks)")

        # 3. ‡πÉ‡∏™‡πà Metadata
        for chunk in chunks:
            chunk.metadata["added_by"] = user_name
            chunk.metadata["timestamp"] = str(datetime.now())
            chunk.metadata["source_type"] = "web_url"
            if "source" not in chunk.metadata: 
                chunk.metadata["source"] = url

        # 4. ‡∏ó‡∏¢‡∏≠‡∏¢‡∏™‡πà‡∏á
        batch_size = 30
        sleep_time = 20
        
        for i in range(0, total_chunks, batch_size):
            if training_state["abort"]:
                add_log("‚õî ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ñ‡∏π‡∏Å‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å")
                training_state["status"] = "Cancelled"
                training_state["is_running"] = False
                return

            current_time = time.time()
            elapsed_time = current_time - training_state["start_time"]
            processed = i
            if processed > 0:
                speed = processed / elapsed_time
                remaining_chunks = total_chunks - processed
                eta = remaining_chunks / speed if speed > 0 else 0
                training_state["estimated_remaining"] = int(eta)
            
            percent = int((i / total_chunks) * 100)
            training_state["progress"] = percent
            training_state["status"] = "Processing"
            add_log(f"üì§ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡πà‡∏á Batch {(i//batch_size)+1} (Process: {i}/{total_chunks})")

            batch = chunks[i : i + batch_size]
            vectorstore.add_documents(documents=batch, namespace=namespace)
            
            add_log(f"‚úÖ Batch {(i//batch_size)+1} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! ‡∏û‡∏±‡∏Å {sleep_time} ‡∏ß‡∏¥...")
            
            for _ in range(sleep_time):
                if training_state["abort"]: break
                time.sleep(1)

        training_state["progress"] = 100
        training_state["status"] = "Completed"
        training_state["is_running"] = False
        add_log("üéâ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå! ‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")

    except Exception as e:
        training_state["status"] = "Error"
        training_state["is_running"] = False
        add_log(f"‚ö†Ô∏è Error: {str(e)}")

def process_drive_training(folder_id: str, key_path: str, namespace: str, user_name: str):
    global training_state
    
    # Reset State
    training_state.update({
        "is_running": True,
        "progress": 0,
        "total_chunks": 0,
        "processed_chunks": 0,
        "status": "Starting",
        "logs": [],
        "start_time": time.time(),
        "estimated_remaining": 0,
        "abort": False 
    })

    try:
        add_log(f"üìÅ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Google Drive (Folder ID: {folder_id})")
        
        # -----------------------------------------------------
        # üîß 1. ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Google Drive API ‡πÅ‡∏ö‡∏ö Manual
        # -----------------------------------------------------
        creds = service_account.Credentials.from_service_account_file(
            key_path, scopes=['https://www.googleapis.com/auth/drive.readonly']
        )
        service = build('drive', 'v3', credentials=creds)

        # -----------------------------------------------------
        # üîç 2. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô Folder (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•)
        # -----------------------------------------------------
        results = service.files().list(
            q=f"'{folder_id}' in parents and trashed=false",
            fields="files(id, name, mimeType)",
            pageSize=1000
        ).execute()
        
        items = results.get('files', [])
        add_log(f"‚úÖ ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(items)} ‡πÑ‡∏ü‡∏•‡πå (‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡∏±‡∏î‡∏Å‡∏£‡∏≠‡∏á...)")

        docs = []
        
        for item in items:
            # ‡πÄ‡∏ä‡πá‡∏Ñ Cancel
            if training_state["abort"]: break
            
            file_id = item['id']
            name = item['name']
            mime = item['mimeType']
            content = ""

            try:
                # üìÑ Case A: ‡πÄ‡∏õ‡πá‡∏ô Google Docs (‡∏ï‡πâ‡∏≠‡∏á Export ‡πÄ‡∏õ‡πá‡∏ô Text)
                if mime == 'application/vnd.google-apps.document':
                    add_log(f"   üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏õ‡∏•‡∏á G-Doc: {name}")
                    request = service.files().export_media(fileId=file_id, mimeType='text/plain')
                    content = request.execute().decode('utf-8')

                # üìù Case B: ‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå Text/Markdown (.md, .txt, .json, .py, etc.)
                # ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà MIME type ‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ text/
                elif name.endswith(('.md', '.txt', '.json', '.py', '.js', '.csv')) or mime.startswith('text/'):
                    add_log(f"   ‚¨áÔ∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå: {name}")
                    request = service.files().get_media(fileId=file_id)
                    fh = io.BytesIO()
                    downloader = MediaIoBaseDownload(fh, request)
                    done = False
                    while done is False:
                        status, done = downloader.next_chunk()
                    
                    fh.seek(0)
                    content = fh.read().decode('utf-8', errors='ignore') # ignore error ‡∏†‡∏≤‡∏©‡∏≤‡∏ï‡πà‡∏≤‡∏á‡∏î‡∏≤‡∏ß
                
                else:
                    # ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å (‡πÄ‡∏ä‡πà‡∏ô ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û, ‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠)
                    add_log(f"   ‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏ü‡∏•‡πå: {name} (‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó {mime} ‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö)")
                    continue

                # ‡∏™‡∏£‡πâ‡∏≤‡∏á Document Object ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
                if content.strip():
                    doc = Document(
                        page_content=content,
                        metadata={"source": name, "title": name, "file_id": file_id}
                    )
                    docs.append(doc)

            except Exception as e:
                add_log(f"   ‚ùå ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå {name} ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {str(e)}")

        # -----------------------------------------------------
        
        if not docs:
            add_log("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢")
            training_state["status"] = "Failed"
            training_state["is_running"] = False
            return

        add_log(f"‚úÖ ‡πÑ‡∏î‡πâ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏ó‡∏£‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(docs)} ‡∏â‡∏ö‡∏±‡∏ö")

        # 3. ‡∏´‡∏±‡πà‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Splitting) - Logic ‡πÄ‡∏î‡∏¥‡∏°
        add_log(f"‚úÇÔ∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏´‡∏±‡πà‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤...")
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        chunks = text_splitter.split_documents(docs)
        
        total_chunks = len(chunks)
        training_state["total_chunks"] = total_chunks
        add_log(f"üì¶ ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {total_chunks} ‡∏ä‡∏¥‡πâ‡∏ô")

        # 4. ‡πÉ‡∏™‡πà Metadata ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°
        for chunk in chunks:
            chunk.metadata["added_by"] = user_name
            chunk.metadata["timestamp"] = str(datetime.now())
            chunk.metadata["source_type"] = "google_drive"
            chunk.metadata["folder_id"] = folder_id

        # 5. ‡∏ó‡∏¢‡∏≠‡∏¢‡∏™‡πà‡∏á (Loop ‡πÄ‡∏î‡∏¥‡∏°)
        batch_size = 30
        sleep_time = 20
        
        for i in range(0, total_chunks, batch_size):
            if training_state["abort"]:
                add_log("‚õî ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ñ‡∏π‡∏Å‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å")
                training_state["status"] = "Cancelled"
                training_state["is_running"] = False
                return

            current_time = time.time()
            elapsed_time = current_time - training_state["start_time"]
            processed = i
            if processed > 0:
                speed = processed / elapsed_time
                remaining_chunks = total_chunks - processed
                eta = remaining_chunks / speed if speed > 0 else 0
                training_state["estimated_remaining"] = int(eta)
            
            percent = int((i / total_chunks) * 100)
            training_state["progress"] = percent
            training_state["status"] = "Processing"
            add_log(f"üì§ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡πà‡∏á Batch {(i//batch_size)+1} (Process: {i}/{total_chunks})")

            batch = chunks[i : i + batch_size]
            vectorstore.add_documents(documents=batch, namespace=namespace)
            
            add_log(f"‚úÖ Batch {(i//batch_size)+1} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! ‡∏û‡∏±‡∏Å {sleep_time} ‡∏ß‡∏¥...")
            
            for _ in range(sleep_time):
                if training_state["abort"]: break
                time.sleep(1)

        training_state["progress"] = 100
        training_state["status"] = "Completed"
        training_state["is_running"] = False
        add_log("üéâ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå! Google Drive ‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")

    except Exception as e:
        training_state["status"] = "Error"
        training_state["is_running"] = False
        add_log(f"‚ö†Ô∏è Error: {str(e)}")      

def process_github_training(repo_name: str, token: str, namespace: str, user_name: str, incremental: bool = False):
    global training_state
    
    # Reset State
    training_state.update({
        "is_running": True,
        "progress": 0,
        "total_chunks": 0,
        "processed_chunks": 0,
        "status": "Starting",
        "logs": [],
        "start_time": time.time(),
        "estimated_remaining": 0,
        "abort": False  # ‚úÖ 1. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ò‡∏á‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å
    })

    try:
        add_log(f"üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ GitHub Repo: {repo_name}")
        if incremental:
            add_log(f"üîÑ Mode: Incremental Update (‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÉ‡∏ô 30 ‡∏ß‡∏±‡∏ô)")
        else:
            add_log(f"üíø Mode: Full Load (‡πÇ‡∏´‡∏•‡∏î‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)")

        # 1. ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ GitHub
        if token: g = Github(token)
        else: g = Github()
        repo = g.get_repo(repo_name)

        docs = []
        
        # ‚úÖ ‡πÅ‡∏¢‡∏Å Logic ‡∏ï‡∏≤‡∏°‡πÇ‡∏´‡∏°‡∏î
        if incremental:
            # 1.1 ‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô
            file_paths = get_modified_files(repo, days=30)
            if not file_paths:
                add_log("‚úÖ ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÉ‡∏´‡∏°‡πà‡πÜ ‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á 30 ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏°‡∏≤")
                training_state["status"] = "Completed"
                training_state["progress"] = 100
                training_state["is_running"] = False
                return
            
            # 1.2 ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå
            add_log(f"üì• ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î {len(file_paths)} ‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà...")
            for idx, path in enumerate(file_paths):
                # üõë ‡πÄ‡∏ä‡πá‡∏Ñ Cancel ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå (‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏¢‡∏≠‡∏∞)
                if training_state["abort"]:
                    add_log("‚õî ‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡∏Å‡∏≤‡∏£‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î")
                    training_state["status"] = "Cancelled"
                    training_state["is_running"] = False
                    return

                doc = get_file_content(repo, path)
                if doc: docs.append(doc)
        else:
            # 1.1 ‡πÇ‡∏´‡∏•‡∏î‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (Logic ‡πÄ‡∏î‡∏¥‡∏°)
            docs = get_github_docs(repo_name, token)

        if not docs:
            add_log("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•")
            training_state["status"] = "Failed"
            training_state["is_running"] = False
            return

        add_log(f"‚úÇÔ∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏´‡∏±‡πà‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å {len(docs)} ‡πÑ‡∏ü‡∏•‡πå...")
        
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        chunks = text_splitter.split_documents(docs)
        
        total_chunks = len(chunks)
        training_state["total_chunks"] = total_chunks
        add_log(f"üì¶ ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {total_chunks} ‡∏ä‡∏¥‡πâ‡∏ô (Chunks)")

        for chunk in chunks:
            chunk.metadata["added_by"] = user_name
            chunk.metadata["timestamp"] = str(datetime.now())
            chunk.metadata["source_type"] = "github_repo"

        batch_size = 30  
        sleep_time = 20  
        
        for i in range(0, total_chunks, batch_size):
            # üõë 2. ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ò‡∏á‡πÅ‡∏î‡∏á ‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ Batch
            if training_state["abort"]:
                add_log("‚õî ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ñ‡∏π‡∏Å‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡πÇ‡∏î‡∏¢ Admin")
                training_state["status"] = "Cancelled"
                training_state["is_running"] = False
                return # ‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏ß‡∏•‡∏≤
            current_time = time.time()
            elapsed_time = current_time - training_state["start_time"]
            processed = i
            if processed > 0:
                speed = processed / elapsed_time
                remaining_chunks = total_chunks - processed
                eta = remaining_chunks / speed if speed > 0 else 0
                training_state["estimated_remaining"] = int(eta)
            
            percent = int((i / total_chunks) * 100)
            training_state["progress"] = percent
            training_state["processed_chunks"] = i
            training_state["status"] = "Processing"
            
            add_log(f"üì§ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡πà‡∏á Batch {(i//batch_size)+1} (Process: {i}/{total_chunks}) - ETA: {int(training_state['estimated_remaining'])}s")

            batch = chunks[i : i + batch_size]
            vectorstore.add_documents(documents=batch, namespace=namespace)
            
            add_log(f"‚úÖ Batch {(i//batch_size)+1} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! ‡∏û‡∏±‡∏Å‡∏´‡∏≤‡∏¢‡πÉ‡∏à {sleep_time} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ...")
            
            # üõë 3. Smart Sleep (‡πÄ‡∏ä‡πá‡∏Ñ Cancel ‡∏ó‡∏∏‡∏Å‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏û‡∏±‡∏Å)
            for _ in range(sleep_time):
                if training_state["abort"]: break
                time.sleep(1)
            
        training_state["progress"] = 100
        training_state["status"] = "Completed"
        training_state["is_running"] = False
        add_log(f"üéâ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå! ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {total_chunks} ‡∏ä‡∏¥‡πâ‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")
        
    except Exception as e:
        training_state["status"] = "Error"
        training_state["is_running"] = False
        add_log(f"‚ö†Ô∏è Error: {str(e)}")

@app.post("/train/cancel")
async def cancel_training(current_user: UserModel = Depends(get_current_user)):
    global training_state
    if training_state["is_running"]:
        training_state["abort"] = True
        training_state["status"] = "Cancelling..."
        add_log("üõë ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å! ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£...")
    return {"status": "success", "message": "Cancellation requested"}        

# ‚úÖ API ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö‡∏°‡∏≤‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
@app.get("/train/status")
async def get_training_status():
    return training_state

class GithubRequest(BaseModel):
    repo_name: str
    github_token: str
    namespace: str = "global"
    incremental: bool = False 

@app.post("/train/github")
async def train_github(
    request: GithubRequest,
    background_tasks: BackgroundTasks,
    current_user: UserModel = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    if current_user.client_id != "global" and request.namespace != current_user.client_id:
         raise HTTPException(status_code=403, detail="‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå")

    background_tasks.add_task(
        process_github_training, 
        request.repo_name, 
        request.github_token, 
        request.namespace, 
        current_user.username,
        request.incremental
    )
    
    mode_text = "Incremental Update" if request.incremental else "Full Load"
    return {"status": "success", "message": f"‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ {mode_text} ‡πÅ‡∏•‡πâ‡∏ß!"}

class UrlRequest(BaseModel):
    url: str
    namespace: str = "global"
    recursive: bool = False
    depth: int = 2

@app.post("/train/url")
async def train_url(
    request: UrlRequest,
    background_tasks: BackgroundTasks,
    current_user: UserModel = Depends(get_current_user)
):
    if current_user.client_id != "global" and request.namespace != current_user.client_id:
         raise HTTPException(status_code=403, detail="‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå")

    background_tasks.add_task(
        process_url_training, 
        request.url, 
        request.namespace, 
        current_user.username,
        request.recursive,
        request.depth
    )
    return {"status": "success", "message": "Start processing URL"}

@app.post("/train/drive")
async def train_drive(
    background_tasks: BackgroundTasks,  # ‚úÖ ‡∏¢‡πâ‡∏≤‡∏¢‡∏°‡∏≤‡πÑ‡∏ß‡πâ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÅ‡∏£‡∏Å‡∏™‡∏∏‡∏î (‡∏Å‡πà‡∏≠‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ =)
    folder_id: str = Form(...),
    namespace: str = Form(...),
    file: UploadFile = File(...), 
    current_user: UserModel = Depends(get_current_user)
):
    if current_user.client_id != "global" and namespace != current_user.client_id:
         raise HTTPException(status_code=403, detail="‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå")

    # 1. ‡πÄ‡∏ã‡∏ü‡πÑ‡∏ü‡∏•‡πå Key ‡∏•‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á Server ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
    key_filename = f"service_key_{current_user.username}.json" # (‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏¥‡∏î‡∏´‡∏ô‡πà‡∏≠‡∏¢‡∏Å‡∏±‡∏ô‡∏ä‡∏ô‡∏Å‡∏±‡∏ô)
    with open(key_filename, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    
    # 2. ‡∏™‡∏±‡πà‡∏á‡∏£‡∏±‡∏ô Background Task
    background_tasks.add_task(
        process_drive_training, 
        folder_id, 
        key_filename, 
        namespace, 
        current_user.username
    )
    
    return {"status": "success", "message": "Start processing Google Drive"}

# --- üõ†Ô∏è Debug / Reset DB API ---
@app.get("/debug/init-db")
async def init_database_endpoint(db: Session = Depends(get_db)):
    try:
        print("üöÄ Resetting Database via API...")
        Base.metadata.drop_all(bind=engine)
        Base.metadata.create_all(bind=engine)

        users_data = [
            ("manager_seaside", "1234", "hotel-seaside"),
            ("manager_city", "1234", "hotel-city"),
            ("admin", "admin", "global")
        ]
        
        created_users = []
        for username, pwd, ns in users_data:
            new_user = UserModel(
                username=username,
                hashed_password=get_password_hash(pwd),
                client_id=ns,
                full_name=username 
            )
            db.add(new_user)
            created_users.append(username)
        
        db.commit()
        
        return {
            "status": "success", 
            "message": "üéâ Database Reset & Initialized Successfully!", 
            "users_created": created_users
        }

    except Exception as e:
        print(f"‚ùå Error: {e}")
        return {"status": "error", "message": str(e)}
    
    

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8000))
    uvicorn.run(app, host="0.0.0.0", port=port)