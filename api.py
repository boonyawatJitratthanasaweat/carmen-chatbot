from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import os
import uvicorn
# ‡πÉ‡∏ä‡πâ Library ‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_pinecone import PineconeVectorStore
from langchain.chains import RetrievalQA
from langchain_core.prompts import PromptTemplate # ‡∏¢‡πâ‡∏≤‡∏¢‡∏°‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏à‡∏≤‡∏Å Core
from fastapi.middleware.cors import CORSMiddleware

# --- ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ App ---
app = FastAPI()

# ‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡πÉ‡∏´‡πâ‡πÄ‡∏ß‡πá‡∏ö‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Config (‡πÉ‡∏™‡πà Key ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì) ---

INDEX_NAME = "docscarmencloud"

# --- ‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏°‡∏≠‡∏á (Global Variable) ---
print("üß† ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏°‡∏≠‡∏á AI... (Gemini + Pinecone)")

try:
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
    vectorstore = PineconeVectorStore(index_name=INDEX_NAME, embedding=embeddings)
    llm = ChatGoogleGenerativeAI(
   model="gemma-3-27b-it", 
    temperature=0.3,
    google_api_key=os.environ["GOOGLE_API_KEY"]
)

    prompt_template = """
    "You are a helpful female assistant named Carmen. Always answer in Thai using polite female particles (‡∏Ñ‡πà‡∏∞/‡∏Ñ‡∏∞)."
    ‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô AI Support ‡∏Ç‡∏≠‡∏á CARMEN 
    ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà: ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
    
    ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á:
    {context}
    
    ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}
    
    ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢, ‡∏™‡∏∏‡∏†‡∏≤‡∏û, ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö):
    """
    PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
        chain_type_kwargs={"prompt": PROMPT}
    )
    print("‚úÖ ‡∏™‡∏°‡∏≠‡∏á‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡πâ‡∏ß!")
except Exception as e:
    print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ï‡∏≠‡∏ô‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏°‡∏≠‡∏á: {e}")

# --- ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ---
class Question(BaseModel):
    text: str

# --- ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏ï‡∏π‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ---
@app.post("/chat")
async def chat_endpoint(question: Question):
    if not qa_chain:
        raise HTTPException(status_code=500, detail="AI ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
    
    try:
        # ‡∏™‡∏±‡πà‡∏á‡πÉ‡∏´‡πâ AI ‡∏ï‡∏≠‡∏ö
        response = qa_chain.invoke(question.text)
        return {"answer": response['result']}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏ñ‡πâ‡∏≤‡∏™‡∏±‡πà‡∏á‡∏£‡∏±‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡∏ï‡∏£‡∏á‡πÜ ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏¥‡πà‡∏° Server ‡πÄ‡∏•‡∏¢
if __name__ == "__main__":
    import uvicorn
    # ‡∏î‡∏∂‡∏á Port ‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏ö (Render ‡∏à‡∏∞‡∏™‡πà‡∏á‡∏°‡∏≤‡πÉ‡∏´‡πâ‡πÄ‡∏≠‡∏á) ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ 8000
    port = int(os.environ.get("PORT", 8000))
    uvicorn.run(app, host="0.0.0.0", port=port)