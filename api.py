from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import os
import uvicorn
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_pinecone import PineconeVectorStore
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from fastapi.middleware.cors import CORSMiddleware
from dotenv import load_dotenv

# ‡πÇ‡∏´‡∏•‡∏î Environment Variables
load_dotenv()

# --- ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ App ---
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Config ---
INDEX_NAME = "docscarmencloud" # ‡∏ä‡∏∑‡πà‡∏≠ Index ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì

# --- ‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏°‡∏≠‡∏á (Global Variable) ---
print("üß† ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏°‡∏≠‡∏á AI... (Gemini + Pinecone)")

try:
    # 1. Setup Embeddings & LLM
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
    
    # ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏° Pinecone (‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î namespace ‡∏ï‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ)
    vectorstore = PineconeVectorStore(index_name=INDEX_NAME, embedding=embeddings)
    
    llm = ChatGoogleGenerativeAI(
        model="gemma-3-27b-it", 
        temperature=0.3,
        google_api_key=os.environ["GOOGLE_API_KEY"]
    )

    # 2. Setup Prompt
    prompt_template = """
    "You are a helpful female assistant named Carmen. Always answer in Thai using polite female particles (‡∏Ñ‡πà‡∏∞/‡∏Ñ‡∏∞)."
    ‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô AI Support ‡∏Ç‡∏≠‡∏á CARMEN 
    ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà: ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
    
    ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á:
    {context}
    
    ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}
    
    ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢, ‡∏™‡∏∏‡∏†‡∏≤‡∏û, ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö):
    """
    PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

    print("‚úÖ ‡∏™‡∏°‡∏≠‡∏á‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡πâ‡∏ß!")

except Exception as e:
    print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ï‡∏≠‡∏ô‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏°‡∏≠‡∏á: {e}")
    vectorstore = None
    llm = None

# --- ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° (‡πÄ‡∏û‡∏¥‡πà‡∏° client_id) ---
class Question(BaseModel):
    text: str
    client_id: str = "" # ‡∏Ñ‡πà‡∏≤ Default ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á

# --- Helper Function: ‡∏£‡∏ß‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ ---
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# --- ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏ï‡∏π‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ---
@app.post("/chat")
async def chat_endpoint(question: Question):
    if not vectorstore or not llm:
        raise HTTPException(status_code=500, detail="AI ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô (Init Failed)")
    
    try:
        user_message = question.text
        client_ns = question.client_id.strip() # ‡∏ï‡∏±‡∏î‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏á
        
        print(f"üì© ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {user_message} | üè¢ Namespace: '{client_ns}'")

        # ---------------------------------------------------------
        # üîç ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô Namespace ‡∏Ç‡∏≠‡∏á‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤ (Private)
        # ---------------------------------------------------------
        docs = []
        if client_ns:
            try:
                print(f"   running search in: {client_ns}")
                docs = vectorstore.similarity_search(
                    user_message, 
                    k=3, 
                    namespace=client_ns
                )
            except Exception as ns_err:
                print(f"   ‚ö†Ô∏è Warning searching namespace: {ns_err}")

        # ---------------------------------------------------------
        # üîç ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ (‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ client_id) ‡πÉ‡∏´‡πâ‡∏´‡∏≤‡πÉ‡∏ô Global
        # ---------------------------------------------------------
        if not docs:
            print("   üö´ ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß -> ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô Global (Default)")
            # Pinecone Default Namespace ‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á ""
            docs = vectorstore.similarity_search(
                user_message, 
                k=3, 
                namespace="" 
            )

        # ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡∏ó‡∏±‡πâ‡∏á 2 ‡∏ó‡∏µ‡πà‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡∏≠‡∏∞‡πÑ‡∏£‡πÄ‡∏•‡∏¢
        if not docs:
            return {"answer": "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞ ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ô‡∏µ‡πâ‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡πà‡∏∞"}

        # ---------------------------------------------------------
        # üß† ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ AI ‡∏ï‡∏≠‡∏ö (RAG)
        # ---------------------------------------------------------
        # ‡πÅ‡∏õ‡∏•‡∏á Docs ‡πÄ‡∏õ‡πá‡∏ô Text ‡∏Å‡πâ‡∏≠‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
        context_text = format_docs(docs)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Chain ‡πÅ‡∏ö‡∏ö Manual (‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô‡∏Å‡∏ß‡πà‡∏≤ RetrievalQA)
        chain = PROMPT | llm | StrOutputParser()
        
        # ‡∏£‡∏±‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á
        response = chain.invoke({"context": context_text, "question": user_message})
        
        return {"answer": response}

    except Exception as e:
        print(f"‚ùå Error: {e}")
        raise HTTPException(status_code=500, detail=f"Internal Server Error: {str(e)}")

# ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏ñ‡πâ‡∏≤‡∏™‡∏±‡πà‡∏á‡∏£‡∏±‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡∏ï‡∏£‡∏á‡πÜ ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏¥‡πà‡∏° Server ‡πÄ‡∏•‡∏¢
if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8000))
    uvicorn.run(app, host="0.0.0.0", port=port)